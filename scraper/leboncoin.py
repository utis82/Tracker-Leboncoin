try:
    from firecrawl import Firecrawl  # type: ignore
except ImportError:
    from firecrawl.firecrawl import FirecrawlApp as Firecrawl  # type: ignore
import time
import os
from urllib.parse import quote_plus
from typing import List, Dict, Optional
from pathlib import Path
from dotenv import load_dotenv

# Charge explicitement le .env situ√© √† la racine du projet
PROJECT_ROOT = Path(__file__).resolve().parents[1]
load_dotenv(PROJECT_ROOT / ".env")


class LeboncoinScraper:
    """Scraper pour les annonces Leboncoin via Firecrawl"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        page_retries: int = 3,
        retry_delay: float = 3.0,
    ):
        """
        Initialise le scraper Firecrawl
        
        Args:
            api_key: Cl√© API Firecrawl (ou via variable d'environnement FIRECRAWL_API_KEY)
            page_retries: Nombre de tentatives par page avant d'abandonner
            retry_delay: Temps (en secondes) entre deux tentatives
        """
        if api_key is None:
            api_key = os.getenv('FIRECRAWL_API_KEY')
        
        if not api_key:
            raise ValueError(
                "FIRECRAWL_API_KEY introuvable. D√©finissez la variable d'environnement "
                "ou ajoutez-la dans un fichier .env (FIRECRAWL_API_KEY=...)."
            )
        
        self.fc = Firecrawl(api_key=api_key)
        self.base_url = "https://www.leboncoin.fr/recherche"
        self.page_retries = max(1, page_retries)
        self.retry_delay = max(0.0, retry_delay)
        
    def build_url(self, model: str, year_min: int, year_max: int, page: int = 1) -> str:
        """
        Construit l'URL de recherche Leboncoin
        
        Args:
            model: Mod√®le de moto (ex: "triumph street triple 765 rs")
            year_min: Ann√©e minimum
            year_max: Ann√©e maximum
            page: Num√©ro de page
            
        Returns:
            URL compl√®te
        """
        params = {
            'category': '3',  # Cat√©gorie motos
            'text': model.strip(),
            'regdate': f"{year_min}-{year_max}",
            'moto_type': 'moto'
        }
        
        # Construction manuelle de l'URL pour plus de contr√¥le
        query_string = f"category={params['category']}"
        query_string += f"&text={quote_plus(params['text'])}"
        query_string += f"&regdate={params['regdate']}"
        query_string += f"&moto_type={params['moto_type']}"
        
        if page > 1:
            query_string += f"&page={page}"
        
        return f"{self.base_url}?{query_string}"
    
    def scrape_page(self, url: str) -> List[Dict]:
        """
        Scrape une page d'annonces
        
        Args:
            url: URL √† scraper
            
        Returns:
            Liste de dictionnaires contenant les donn√©es d'annonces
        """
        print(f"üì° Scraping: {url}")

        # Schema enrichi pour extraire plus de donn√©es
        schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "Le titre complet de l'annonce"
                    },
                    "price": {
                        "type": "string",
                        "description": "Le prix en euros (avec ou sans le symbole ‚Ç¨)"
                    },
                    "mileage": {
                        "type": "string",
                        "description": "Le kilom√©trage (avec ou sans 'km')"
                    },
                    "year": {
                        "type": ["string", "integer"],
                        "description": "L'ann√©e de mise en circulation"
                    },
                    "location": {
                        "type": "string",
                        "description": "La ville ou localisation"
                    },
                    "link": {
                        "type": "string",
                        "description": "Le lien complet vers l'annonce"
                    },
                    "photo": {
                        "type": "string",
                        "description": "L'URL de la premi√®re photo de l'annonce"
                    }
                },
                "required": ["price", "mileage"]
            }
        }

        last_error: Optional[Exception] = None
        for attempt in range(1, self.page_retries + 1):
            if attempt > 1:
                print(f"‚Üª Nouvelle tentative {attempt}/{self.page_retries} apr√®s erreur pr√©c√©dente...")
            try:
                doc = self.fc.scrape(
                    url,
                    formats=[{
                        "type": "json",
                        "prompt": """Extract all motorcycle ads from this Leboncoin page.
                        For each ad, extract:
                        - title (full listing title)
                        - price (in euros)
                        - mileage (in km)
                        - year (registration year)
                        - location (city)
                        - link (full URL to the ad)
                        - photo (URL of the main image)
                        
                        Return an array of all ads found on the page.""",
                        "schema": schema
                    }]
                )

                if hasattr(doc, 'json') and doc.json:
                    ads = doc.json if isinstance(doc.json, list) else [doc.json]
                    print(f"‚úÖ {len(ads)} annonces extraites")
                    return ads

                # Pas d'exception mais aucune donn√©e: on tente √† nouveau
                print("‚ö†Ô∏è Aucune donn√©e JSON retourn√©e par Firecrawl sur cette tentative")
            except Exception as e:
                last_error = e
                print(f"‚ö†Ô∏è Erreur Firecrawl tentative {attempt}/{self.page_retries}: {e}")

            if attempt < self.page_retries:
                time.sleep(self.retry_delay)

        if last_error:
            print(f"‚ùå Erreur lors du scraping apr√®s {self.page_retries} tentatives: {last_error}")
        else:
            print("‚ö†Ô∏è Firecrawl n'a renvoy√© aucune donn√©e apr√®s plusieurs tentatives")
        return []
    
    def scrape(self, model: str, year_min: int, year_max: int, max_pages: int = 3) -> List[Dict]:
        """
        Scrape plusieurs pages d'annonces
        
        Args:
            model: Mod√®le de moto
            year_min: Ann√©e minimum
            year_max: Ann√©e maximum
            max_pages: Nombre maximum de pages √† scraper
            
        Returns:
            Liste compl√®te des annonces
        """
        all_ads = []
        
        print(f"\nüîç Recherche: {model} ({year_min}-{year_max})")
        print(f"üìÑ Pages √† scanner: {max_pages}\n")
        
        for page in range(1, max_pages + 1):
            url = self.build_url(model, year_min, year_max, page)
            ads = self.scrape_page(url)
            
            if ads:
                all_ads.extend(ads)
                print(f"üìä Total accumul√©: {len(all_ads)} annonces\n")
            else:
                print(f"‚ö†Ô∏è Page {page} vide ou erreur, arr√™t du scraping\n")
                break
            
            # Pause entre les pages pour √©viter la surcharge
            if page < max_pages:
                time.sleep(1)
        
        print(f"‚úÖ Scraping termin√©: {len(all_ads)} annonces au total\n")
        return all_ads
    
    def scrape_with_retry(self, model: str, year_min: int, year_max: int, 
                          max_pages: int = 3, max_retries: int = 3) -> List[Dict]:
        """
        Scrape avec m√©canisme de retry en cas d'√©chec
        
        Args:
            model: Mod√®le de moto
            year_min: Ann√©e minimum
            year_max: Ann√©e maximum
            max_pages: Nombre maximum de pages
            max_retries: Nombre maximum de tentatives
            
        Returns:
            Liste des annonces
        """
        for attempt in range(1, max_retries + 1):
            try:
                ads = self.scrape(model, year_min, year_max, max_pages)
                if ads:
                    return ads
                
                if attempt < max_retries:
                    print(f"‚ö†Ô∏è Tentative {attempt}/{max_retries} √©chou√©e, nouvelle tentative dans 3s...")
                    time.sleep(3)
                    
            except Exception as e:
                print(f"‚ùå Erreur tentative {attempt}/{max_retries}: {str(e)}")
                if attempt < max_retries:
                    time.sleep(3)
        
        print("‚ùå √âchec apr√®s toutes les tentatives")
        return []


# Test du module
if __name__ == "__main__":
    scraper = LeboncoinScraper()
    
    # Test avec la Street Triple RS
    results = scraper.scrape(
        model="triumph street triple 765 rs",
        year_min=2017,
        year_max=2020,
        max_pages=2
    )
    
    print("\nüìã R√©sum√© des r√©sultats:")
    print(f"Nombre d'annonces: {len(results)}")
    
    if results:
        print("\nüîç Premi√®re annonce:")
        for key, value in results[0].items():
            print(f"  {key}: {value}")
